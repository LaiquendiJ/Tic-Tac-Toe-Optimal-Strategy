{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "playerA = 1\n",
    "playerB = -1\n",
    "empty = 0\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, player1=True, epsilon=1, eps_dec=0.000001, eps_min=0.08, learning_rate=0.1, discount_factor=0.9):\n",
    "        if player1:\n",
    "            self.player = 1\n",
    "        else:\n",
    "            self.player = -1\n",
    "        self.epsilon = epsilon  \n",
    "        self.epsilon_min = eps_min\n",
    "        self.epsilon_dec = eps_dec\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.Q_table = {}\n",
    "        self.visited = []  # tracks visited states in the episode\n",
    "\n",
    "    def select_action(self, state):\n",
    "        t_state = tuple(state)\n",
    "        action = 0\n",
    "        legal_moves = [idx for idx, i in enumerate(t_state) if i == 0]\n",
    "        if (t_state, 0) not in self.Q_table:  \n",
    "            for move in range(9):\n",
    "                if (t_state, move) not in self.Q_table:\n",
    "                    # adding possible moves\n",
    "                    self.Q_table[(t_state, move)] = 0.05\n",
    "\n",
    "        if random.random() > self.epsilon:  # if choosing 'optimal' move\n",
    "            action_scores = [self.Q_table[t_state, move]\n",
    "                             for move in legal_moves]  # get each legal action\n",
    "            # get the best 'legal' action\n",
    "            action_pos = np.argmax(action_scores)\n",
    "            action = legal_moves[action_pos]  # find which action it relates to\n",
    "        else:  # if random move\n",
    "            action = random.choice(legal_moves)\n",
    "        # store a list of 'visited' nodes \n",
    "        self.visited.append((t_state, action))\n",
    "        return action\n",
    "\n",
    "    def minimax(self, depth, player):\n",
    "        if depth == 0 or self.game_over():\n",
    "            return [[], self.evaluate()]\n",
    "\n",
    "        if player == self.playerA:\n",
    "            best = [[], -math.inf]\n",
    "        else:\n",
    "            best = [[], math.inf]\n",
    "\n",
    "        for move in self.valid_moves():\n",
    "            row, col = move\n",
    "            self.board[row][col] = player  # Make the move\n",
    "            _, score = self.minimax(depth - 1, -player)\n",
    "            self.board[row][col] = self.empty  # Undo the move\n",
    "\n",
    "            if player == self.playerA:\n",
    "                if score > best[1]:\n",
    "                    best = [[move], score]\n",
    "                elif score == best[1]:\n",
    "                    best[0].append(move)\n",
    "            else:\n",
    "                if score < best[1]:\n",
    "                    best = [[move], score]\n",
    "                elif score == best[1]:\n",
    "                    best[0].append(move)\n",
    "\n",
    "        return best\n",
    "\n",
    "    def learn(self, winner):\n",
    "        # Reward\n",
    "        reward = 1 if winner == self.player else (-1 if winner != 0 else -0.1)\n",
    "\n",
    "        # Update states before the end\n",
    "        for idx, (state, action) in enumerate(self.visited[:-1]):\n",
    "            next_state, _ = self.visited[idx+1]\n",
    "            max_Q = max([self.Q_table[(next_state, a)] for a in range(9)])\n",
    "            self.Q_table[(state, action)] = self.Q_table[(state, action)] + \\\n",
    "                self.learning_rate * \\\n",
    "                (reward + self.discount_factor *\n",
    "                 max_Q - self.Q_table[(state, action)])\n",
    "\n",
    "        # Update the last state\n",
    "        (last_state, last_action) = self.visited[-1]  # check update\n",
    "        self.Q_table[(last_state, last_action)] = self.Q_table[(last_state,\n",
    "                                                                last_action)] + self.learning_rate * (reward - self.Q_table[(last_state, last_action)])\n",
    "\n",
    "        # clearing states visited for the episode\n",
    "        self.visited = []\n",
    "        self.epsilon -= self.epsilon_dec\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "\n",
    "class Board:\n",
    "    def __init__(self):\n",
    "        # A flattened representation of the 3x3 tic-tac-toe board\n",
    "        self.board = [0] * 9\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the board to its initial state.\"\"\"\n",
    "        self.board = [0] * 9\n",
    "\n",
    "    def place_piece(self, position, player1=True):\n",
    "        \"\"\"Places a piece on the board at the given position.\"\"\"\n",
    "        if self.board[position] != 0:\n",
    "            print(\"Invalid move\")\n",
    "            return False\n",
    "\n",
    "        self.board[position] = 1 if player1 else -1\n",
    "        return True\n",
    "\n",
    "    def game_done(self):\n",
    "        \"\"\"Checks if the game is over (either win or draw).\"\"\"\n",
    "        return self.evaluate() != 0 or self.board.count(0) == 0\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluates the current board state to determine the winner.\"\"\"\n",
    "        winning_combinations = [\n",
    "            (0, 1, 2), (3, 4, 5), (6, 7, 8),\n",
    "            (0, 3, 6), (1, 4, 7), (2, 5, 8),\n",
    "            (0, 4, 8), (2, 4, 6)\n",
    "        ]\n",
    "\n",
    "        for a, b, c in winning_combinations:\n",
    "            if self.board[a] == self.board[b] == self.board[c] and self.board[a] != 0:\n",
    "                return self.board[a]\n",
    "\n",
    "        return 0  # No winner yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1 = Agent()  # player 1 plays O's\n",
    "player2 = Agent(player1=False)  # player 2 plays X's\n",
    "players = [player1, player2]\n",
    "board = Board()\n",
    "vals = []\n",
    "x_val = []\n",
    "x_val_1=[]\n",
    "n_games = 1000000\n",
    "state = (1, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "for game in range(n_games):\n",
    "    board.reset()  # reset board\n",
    "    while not board.game_done():\n",
    "        board.place_piece(player1.select_action(board.board))\n",
    "        if board.game_done():\n",
    "            break\n",
    "        board.place_piece(player2.select_action(\n",
    "            board.board), player1=False)\n",
    "\n",
    "    # Learn from the game\n",
    "    winner = board.evaluate()  \n",
    "    [player.learn(winner) for player in players]\n",
    "\n",
    "    # Store a specific normalized Q-table for future visualization\n",
    "    if game>10000:\n",
    "        for i in range(9):\n",
    "            try:\n",
    "                vals.append(player2.Q_table[(state, i\n",
    "                    )])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        for i in range(9):\n",
    "            vals.append(player2.Q_table[(state, i\n",
    "                )])\n",
    "        if min(vals)!= max(vals):\n",
    "            x_val.append((np.array(vals)-min(vals))/(max(vals)-min(vals)))\n",
    "            vals=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_q_table(state):\n",
    "    \"\"\"\n",
    "    Print the Tic Tac Toe board.\n",
    "    \"\"\"\n",
    "    val = []\n",
    "    \n",
    "    if state.count(1)==state.count(-1):\n",
    "        player = player1\n",
    "    elif state.count(1)==state.count(-1)+1:\n",
    "        player =player2\n",
    "    else:\n",
    "        return('Not a valid state!')\n",
    "        \n",
    "    for i in range(0, 9):\n",
    "        val.append(player.Q_table[(state, i\n",
    "                           )])\n",
    "    table=val\n",
    "    print(f\"The unormalized Q-table for state {state} is:\")\n",
    "    for i in range(3):\n",
    "        print(\" | \".join(str(round(table[(3*i+j)],3))if str(round(table[(3*i+j)],3))!='0.05' else '*'  for j in range(3)))\n",
    "        print(\"-\" * 25)\n",
    "\n",
    "\n",
    "for i in range(1, 9):\n",
    "    x = [j[i] for j in x_val]\n",
    "    x = x[-5000:]\n",
    "\n",
    "    y_val = np.linspace(0, len(x), len(x))\n",
    "    if i ==4:\n",
    "        plt.plot(y_val, x,label='center position')\n",
    "    else:\n",
    "        plt.plot(y_val, x)\n",
    "plt.legend()\n",
    "plt.ylabel('normalized Q-value')\n",
    "plt.xlabel('last 5000 games')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (1, 0, 1, -1, -1, 0, 0, 0, 0)\n",
    "print_q_table(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
